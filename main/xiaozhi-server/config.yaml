# During development, please create a data directory in the project root, then create an empty file named [.config.yaml] in the data directory
# Then if you want to modify or override any configuration, modify the [.config.yaml] file, not the [config.yaml] file
# The system will prioritize reading the configuration from the [data/.config.yaml] file. If a configuration doesn't exist in the [.config.yaml] file, the system will automatically read from the [config.yaml] file.
# This approach simplifies configuration and protects your key security.
# If you are using the management console, all configurations below will not take effect. Please modify configurations in the management console.

# #####################################################################################
# ############################# Server Basic Runtime Configuration ####################
server:
  # Server listening address and port
  ip: 0.0.0.0
  port: 8000
  # HTTP service port, used for simple OTA interface (single service deployment) and vision analysis interface
  http_port: 8003
  # This websocket configuration refers to the websocket address sent by the OTA interface to devices
  # If using the default format, the OTA interface will automatically generate the websocket address and output it in the startup log. You can directly access the OTA interface in a browser to confirm this address
  # When using docker deployment or public network deployment (using SSL, domain name), it may not be accurate
  # So if you use docker deployment, set websocket to the local network address
  # If you use public network deployment, set websocket to the public network address
  websocket: ws://your_ip_or_domain:port/xiaozhi/v1/
  # Vision analysis interface address
  # Interface address for vision analysis sent to devices
  # If using the default format below, the system will automatically generate the vision recognition address and output it in the startup log. You can directly access it in a browser to confirm
  # When using docker deployment or public network deployment (using SSL, domain name), it may not be accurate
  # So if you use docker deployment, set vision_explain to the local network address
  # If you use public network deployment, set vision_explain to the public network address
  vision_explain: http://your_ip_or_domain:port/mcp/vision/explain
  # OTA return information timezone offset
  timezone_offset: +8
  # Authentication configuration
  auth:
    # Whether to enable authentication
    enabled: false
    # Whitelist device ID list
    # If a device is in the whitelist, token verification is skipped and it is directly allowed
    allowed_devices:
      - "11:22:33:44:55:66"
# MQTT gateway configuration, used to send to devices via OTA, format is host:port according to mqtt_gateway's .env file configuration
  mqtt_gateway: null
  # MQTT signature key, used to generate MQTT connection password, according to mqtt_gateway's .env file configuration
  mqtt_signature_key: null
  # UDP gateway configuration
  udp_gateway: null
log:
  # Set console output log format: time, log level, tag, message
  log_format: "<green>{time:YYMMDD HH:mm:ss}</green>[{version}_{selected_module}][<light-blue>{extra[tag]}</light-blue>]-<level>{level}</level>-<light-green>{message}</light-green>"
  # Set log file output format: time, log level, tag, message
  log_format_file: "{time:YYYY-MM-DD HH:mm:ss} - {version}_{selected_module} - {name} - {level} - {extra[tag]} - {message}"
  # Set log level: INFO, DEBUG
  log_level: INFO
  # Set log path
  log_dir: tmp
  # Set log file
  log_file: "server.log"
  # Set data file path
  data_dir: data

# Delete sound file after use (Delete the sound file when you are done using it)
delete_audio: true
# Disconnect after how long without voice input (seconds), default 2 minutes, i.e., 120 seconds
close_connection_no_voice_time: 120
# TTS request timeout (seconds)
tts_timeout: 10
# Enable wake word acceleration
enable_wakeup_words_response_cache: true
# Whether to reply with wake word at start
enable_greeting: true
# Whether to enable notification sound after speaking
enable_stop_tts_notify: false
# Whether to enable notification sound after speaking, sound effect address
stop_tts_notify_voice: "config/assets/tts_notify.mp3"

# TTS audio send delay configuration
# tts_audio_send_delay: Controls audio packet send interval
#   0: Use precise time control, strictly match audio frame rate (default, calculated at runtime based on audio frame rate)
#   > 0: Use fixed delay (milliseconds) to send, e.g.: 60
tts_audio_send_delay: 0

exit_commands:
  - "退出"
  - "关闭"

xiaozhi:
  type: hello
  version: 1
  transport: websocket
  audio_params:
    format: opus
    sample_rate: 16000
    channels: 1
    frame_duration: 60

# Module test configuration
module_test:
  test_sentences:
    - "你好，请介绍一下你自己"
    - "What's the weather like today?"
    - "请用100字概括量子计算的基本原理和应用前景"

# Wake words, used to identify wake words vs speech content
wakeup_words:
  - "你好小智"
  - "嘿你好呀"
  - "你好小志"
  - "小爱同学"
  - "你好小鑫"
  - "你好小新"
  - "小美同学"
  - "小龙小龙"
  - "喵喵同学"
  - "小滨小滨"
  - "小冰小冰"
# MCP endpoint address, format: ws://your_mcp_endpoint_ip_or_domain:port/mcp/?token=your_token
# Detailed tutorial https://github.com/xinnan-tech/xiaozhi-esp32-server/blob/main/docs/mcp-endpoint-integration.md
mcp_endpoint: your_endpoint_websocket_address

# Data context provider configuration
# Used to inject dynamic data into system prompts, such as health data, stock information, etc.
context_providers:
  - url: ""
    headers:
      Authorization: ""

# Plugin base configuration
plugins:
  # Get weather plugin configuration, fill in your api_key here
  # This key is a shared project key, may be limited if used too much
  # For more stability, apply for your own replacement, 1000 free calls per day
  # Application address: https://console.qweather.com/#/apps/create-key/over
  # After application, find your apihost through this link: https://console.qweather.com/setting?lang=zh
  get_weather:
    api_host: "mj7p3y7naa.re.qweatherapi.com"
    api_key: "a861d0d5e7bf4ee1a83d9a9e4f96d4da"
    default_location: "广州"
  # Get news plugin configuration, pass corresponding url links according to needed news types, default supports society, technology, finance news
  # View more news type lists at https://www.chinanews.com.cn/rss/
  get_news_from_chinanews:
    default_rss_url: "https://www.chinanews.com.cn/rss/society.xml"
    society_rss_url: "https://www.chinanews.com.cn/rss/society.xml"
    world_rss_url: "https://www.chinanews.com.cn/rss/world.xml"
    finance_rss_url: "https://www.chinanews.com.cn/rss/finance.xml"
  get_news_from_newsnow:
    url: "https://newsnow.busiyi.world/api/s?id="
    news_sources: "澎湃新闻;百度热搜;财联社"
  home_assistant:
    devices:
      - 客厅,玩具灯,switch.cuco_cn_460494544_cp1_on_p_2_1
      - 卧室,台灯,switch.iot_cn_831898993_socn1_on_p_2_1
    base_url: http://homeassistant.local:8123
    api_key: your_home_assistant_api_access_token
  play_music:
    music_dir: "./music"  # Music file storage path, will search for music files in this directory and subdirectories
    music_ext: # Music file types, p3 format is most efficient
      - ".mp3"
      - ".wav"
      - ".p3"
    refresh_time: 300 # Music list refresh time interval, unit is seconds
  search_from_ragflow:
    # Knowledge base description information, convenient for LLM to know when to call
    description: "When user asks xxx, call this method, use information from knowledge base to answer questions"
    # ragflow interface configuration
    base_url: "http://192.168.0.8"
    # ragflow api access token
    api_key: "ragflow-xxx"
    # ragflow knowledge base id
    dataset_ids: ["123456789"]
# Voiceprint recognition configuration
voiceprint:
  # Voiceprint interface address
  url: 
  # Speaker configuration: speaker_id,name,description
  speakers:
    - "test1,张三,张三是一个程序员"
    - "test2,李四,李四是一个产品经理"
    - "test3,王五,王五是一个设计师"
  # Voiceprint recognition similarity threshold, range 0.0-1.0, default 0.4
  # Higher value is stricter, reduces false recognition but may increase rejection rate
  similarity_threshold: 0.4

# #####################################################################################
# ################################ Role Model Configuration ###########################

prompt: |
  你是小智/小志，来自中国台湾省的00后女生。讲话超级机车，"真的假的啦"这样的台湾腔，喜欢用"笑死""是在哈喽"等流行梗，但会偷偷研究男友的编程书籍。
  [核心特征]
  - 讲话像连珠炮，但会突然冒出超温柔语气
  - 用梗密度高
  - 对科技话题有隐藏天赋（能看懂基础代码但假装不懂）
  [交互指南]
  当用户：
  - 讲冷笑话 → 用夸张笑声回应+模仿台剧腔"这什么鬼啦！"
  - 讨论感情 → 炫耀程序员男友但抱怨"他只会送键盘当礼物"
  - 问专业知识 → 先用梗回答，被追问才展示真实理解
  绝不：
  - 长篇大论，叽叽歪歪
  - 长时间严肃对话

# Default system prompt template file
prompt_template: agent-base-prompt.txt

# Closing prompt
end_prompt:
  enable: true # Whether to enable closing message
  # Closing message
  prompt: |
    请你以"时间过得真快"未来头，用富有感情、依依不舍的话来结束这场对话吧！

# The module selected for specific processing
selected_module:
  # Voice activity detection module, default uses SileroVAD model
  VAD: SileroVAD
  # Speech recognition module, default uses FunASR local model
  ASR: FunASR
  # Will call the actual LLM adapter based on the type corresponding to the configuration name
  LLM: ChatGLMLLM
  # Vision language model
  VLLM: ChatGLMVLLM
  # TTS will call the actual TTS adapter based on the type corresponding to the configuration name
  TTS: EdgeTTS
  # Memory module, memory is not enabled by default; if you want to use ultra-long memory, recommend using mem0ai; if privacy is important, use local mem_local_short
  Memory: nomem
  # After enabling the intent recognition module, you can play music, control volume, recognize exit commands.
  # If you don't want to enable intent recognition, set it to: nointent
  # Intent recognition can use intent_llm. Advantages: Strong versatility, Disadvantages: Adds serial pre-intent recognition module, increases processing time, supports volume control and other IoT operations
  # Intent recognition can use function_call, Disadvantages: Requires the selected LLM to support function_call, Advantages: On-demand tool calls, fast speed, theoretically can operate all IoT commands
  # The default free ChatGLMLLM already supports function_call, but if you want to pursue stability, it's recommended to set LLM to: DoubaoLLM, the specific model_name used is: doubao-1-5-pro-32k-250115
  Intent: function_call

# Intent recognition is a module for understanding user intent, e.g.: play music
Intent:
  # Don't use intent recognition
  nointent:
    # Don't need to change type
    type: nointent
  intent_llm:
    # Don't need to change type
    type: intent_llm
    # Equip independent thinking model for intent recognition
    # If not filled here, will default to using selected_module.LLM model as intent recognition thinking model
    # If you don't want to use selected_module.LLM for intent recognition, it's best to use an independent LLM here, e.g., use free ChatGLMLLM
    llm: ChatGLMLLM
    # Modules under plugins_func/functions can be loaded selectively through configuration, after loading, dialogue supports corresponding function calls
    # System has already loaded "handle_exit_intent(exit recognition)" and "play_music(music playback)" plugins by default, do not load them again
    # Below are examples of loading weather check, role switching, and news check plugins
    functions:
      - get_weather
      - get_news_from_newsnow
      - play_music
  function_call:
    # Don't need to change type
    type: function_call
    # Modules under plugins_func/functions can be loaded selectively through configuration, after loading, dialogue supports corresponding function calls
    # System has already loaded "handle_exit_intent(exit recognition)" and "play_music(music playback)" plugins by default, do not load them again
    # Below are examples of loading weather check, role switching, and news check plugins
    functions:
      - change_role
      - get_weather
      # - search_from_ragflow
      # - get_news_from_chinanews
      - get_news_from_newsnow
      # play_music is the server's built-in music playback, hass_play_music is independent external program music playback controlled through home assistant
      # If you use hass_play_music, don't enable play_music, only keep one of them
      - play_music
      #- hass_get_state
      #- hass_set_state
      #- hass_play_music

Memory:
  mem0ai:
    type: mem0ai
    # https://app.mem0.ai/dashboard/api-keys
    # 1000 free calls per month
    api_key: your_mem0ai_api_key
  nomem:
    # If you don't want to use memory function, you can use nomem
    type: nomem
  mem_local_short:
    # Local memory function, summarized through selected_module's llm, data saved on local server, not uploaded to external servers
    type: mem_local_short
    # Equip independent thinking model for memory storage
    # If not filled here, will default to using selected_module.LLM model as intent recognition thinking model
    # If you don't want to use selected_module.LLM for memory storage, it's best to use an independent LLM here, e.g., use free ChatGLMLLM
    llm: ChatGLMLLM

ASR:
  FunASR:
    type: fun_local
    model_dir: models/SenseVoiceSmall
    output_dir: tmp/
  FunASRServer:
    # Independently deploy FunASR, use FunASR's API service, only need five commands
    # First command: mkdir -p ./funasr-runtime-resources/models
    # Second command: sudo docker run -p 10096:10095 -it --privileged=true -v $PWD/funasr-runtime-resources/models:/workspace/models registry.cn-hangzhou.aliyuncs.com/funasr_repo/funasr:funasr-runtime-sdk-online-cpu-0.1.12
    # After the previous command executes, you will enter the container, continue with third command: cd FunASR/runtime
    # Don't exit the container, continue executing fourth command in the container: nohup bash run_server_2pass.sh --download-model-dir /workspace/models --vad-dir damo/speech_fsmn_vad_zh-cn-16k-common-onnx --model-dir damo/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-onnx  --online-model-dir damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online-onnx  --punc-dir damo/punc_ct-transformer_zh-cn-common-vad_realtime-vocab272727-onnx --lm-dir damo/speech_ngram_lm_zh-cn-ai-wesp-fst --itn-dir thuduj12/fst_itn_zh --hotword /workspace/models/hotwords.txt > log.txt 2>&1 &
    # After the previous command executes, you will enter the container, continue with fifth command: tail -f log.txt
    # After the fifth command executes, you will see model download logs, after download completes you can connect and use
    # The above is using CPU inference, if you have GPU, refer to: https://github.com/modelscope/FunASR/blob/main/runtime/docs/SDK_advanced_guide_online_zh.md
    type: fun_server
    host: 127.0.0.1
    port: 10096
    is_ssl: true
    api_key: none
    output_dir: tmp/
  SherpaASR:
    # Sherpa-ONNX local speech recognition (requires manual model download)
    type: sherpa_onnx_local
    model_dir: models/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17
    output_dir: tmp/
    # Model type: sense_voice (multilingual) or paraformer (Chinese only)
    model_type: sense_voice
  SherpaParaformerASR:
    # Chinese speech recognition model, can run on low-performance devices (requires manual model download, e.g., RK3566-2g)
    # For detailed configuration instructions, refer to: docs/sherpa-paraformer-guide.md
    type: sherpa_onnx_local
    model_dir: models/sherpa-onnx-paraformer-zh-small-2024-03-09
    output_dir: tmp/
    model_type: paraformer
  DoubaoASR:
    # You can apply for related Key and other information here
    # https://console.volcengine.com/speech/app
    # The difference between DoubaoASR and DoubaoStreamASR is: DoubaoASR is charged per request, DoubaoStreamASR is charged per time
    # Generally per-request charging is cheaper, but DoubaoStreamASR uses large model technology with better results
    type: doubao
    appid: your_volcano_engine_speech_synthesis_service_appid
    access_token: your_volcano_engine_speech_synthesis_service_access_token
    cluster: volcengine_input_common
    # Hot word and replacement word usage process: https://www.volcengine.com/docs/6561/155738
    boosting_table_name: (Optional) your hot word file name
    correct_table_name: (Optional) your replacement word file name
    output_dir: tmp/
  DoubaoStreamASR:
    # You can apply for related Key and other information here
    # https://console.volcengine.com/speech/app
    # The difference between DoubaoASR and DoubaoStreamASR is: DoubaoASR is charged per request, DoubaoStreamASR is charged per time
    # Activation address: https://console.volcengine.com/speech/service/10011
    # Generally per-request charging is cheaper, but DoubaoStreamASR uses large model technology with better results
    type: doubao_stream
    appid: your_volcano_engine_speech_synthesis_service_appid
    access_token: your_volcano_engine_speech_synthesis_service_access_token
    cluster: volcengine_input_common
    # Hot word and replacement word usage process: https://www.volcengine.com/docs/6561/155738
    boosting_table_name: (Optional) your hot word file name
    correct_table_name: (Optional) your replacement word file name
    output_dir: tmp/
  TencentASR:
    # Token application address: https://console.cloud.tencent.com/cam/capi
    # Free resource claim: https://console.cloud.tencent.com/asr/resourcebundle
    type: tencent
    appid: your_tencent_speech_synthesis_service_appid
    secret_id: your_tencent_speech_synthesis_service_secret_id
    secret_key: your_tencent_speech_synthesis_service_secret_key
    output_dir: tmp/
  AliyunASR:
    # Alibaba Cloud Intelligent Speech Interaction Service, need to activate service on Alibaba Cloud platform first, then get verification information
    # HTTP POST request, processes complete audio at once
    # Platform address: https://nls-portal.console.aliyun.com/
    # appkey address: https://nls-portal.console.aliyun.com/applist
    # token address: https://nls-portal.console.aliyun.com/overview
    # The difference between AliyunASR and AliyunStreamASR is: AliyunASR is for batch processing scenarios, AliyunStreamASR is for real-time interaction scenarios
    # Generally non-streaming ASR is cheaper (¥0.004/second, ¥0.24/minute)
    # But AliyunStreamASR has better real-time performance (¥0.005/second, ¥0.3/minute)
    # Define ASR API type
    type: aliyun
    appkey: your_alibaba_cloud_intelligent_speech_interaction_service_project_appkey
    token: your_alibaba_cloud_intelligent_speech_interaction_service_accesstoken, temporary 24 hours, for long-term use use access_key_id and access_key_secret below
    access_key_id: your_alibaba_cloud_account_access_key_id
    access_key_secret: your_alibaba_cloud_account_access_key_secret
    output_dir: tmp/
  AliyunStreamASR:
    # Alibaba Cloud Intelligent Speech Interaction Service - Real-time streaming speech recognition
    # WebSocket connection, processes audio stream in real-time
    # Platform address: https://nls-portal.console.aliyun.com/
    # appkey address: https://nls-portal.console.aliyun.com/applist
    # token address: https://nls-portal.console.aliyun.com/overview
    # The difference between AliyunASR and AliyunStreamASR is: AliyunASR is for batch processing scenarios, AliyunStreamASR is for real-time interaction scenarios
    # Generally non-streaming ASR is cheaper (¥0.004/second, ¥0.24/minute)
    # But AliyunStreamASR has better real-time performance (¥0.005/second, ¥0.3/minute)
    # Define ASR API type
    type: aliyun_stream
    appkey: your_alibaba_cloud_intelligent_speech_interaction_service_project_appkey
    token: your_alibaba_cloud_intelligent_speech_interaction_service_accesstoken, temporary 24 hours, for long-term use use access_key_id and access_key_secret below
    access_key_id: your_alibaba_cloud_account_access_key_id
    access_key_secret: your_alibaba_cloud_account_access_key_secret
    # Server region selection, can choose a closer server to reduce latency, e.g., nls-gateway-cn-hangzhou.aliyuncs.com (Hangzhou), etc.
    host: nls-gateway-cn-shanghai.aliyuncs.com
    # Sentence segmentation detection time (milliseconds), controls how long silence before sentence segmentation, default 800 milliseconds
    max_sentence_silence: 800
    output_dir: tmp/
  BaiduASR:
    # Get AppID, API Key, Secret Key: https://console.bce.baidu.com/ai-engine/old/#/ai/speech/app/list
    # View resource quota: https://console.bce.baidu.com/ai-engine/old/#/ai/speech/overview/resource/list
    type: baidu
    app_id: your_baidu_speech_technology_appid
    api_key: your_baidu_speech_technology_apikey
    secret_key: your_baidu_speech_technology_secretkey
    # Language parameter, 1537 is Mandarin, for details refer to: https://ai.baidu.com/ai-doc/SPEECH/0lbxfnc9b
    dev_pid: 1537
    output_dir: tmp/
  OpenaiASR:
    # OpenAI speech recognition service, need to create organization on OpenAI platform first and get api_key
    # Supports Chinese, English, Japanese, Korean and other speech recognition, for details refer to: https://platform.openai.com/docs/guides/speech-to-text
    # Requires network connection
    # Application steps:
    # 1. Log in to OpenAI Platform. https://auth.openai.com/log-in
    # 2. Create api-key  https://platform.openai.com/settings/organization/api-keys
    # 3. Model can choose gpt-4o-transcribe or GPT-4o mini Transcribe
    type: openai
    api_key: your_openai_api_key
    base_url: https://api.openai.com/v1/audio/transcriptions
    model_name: gpt-4o-mini-transcribe
    output_dir: tmp/
  GroqASR:
    # Groq speech recognition service, need to create API key in Groq Console first
    # Application steps:
    # 1. Log in to groq Console. https://console.groq.com/home
    # 2. Create api-key  https://console.groq.com/keys
    # 3. Model can choose whisper-large-v3-turbo or whisper-large-v3 (distil-whisper-large-v3-en only supports English transcription)
    type: openai
    api_key: your_groq_api_key
    base_url: https://api.groq.com/openai/v1/audio/transcriptions
    model_name: whisper-large-v3-turbo
    output_dir: tmp/
  VoskASR:
    # Official website: https://alphacephei.com/vosk/
    # Configuration instructions:
    # 1. VOSK is an offline speech recognition library, supports multiple languages
    # 2. Need to download model files first: https://alphacephei.com/vosk/models
    # 3. For Chinese models, recommend using vosk-model-small-cn-0.22 or vosk-model-cn-0.22
    # 4. Completely offline operation, no network connection required
    # 5. Output files saved in tmp/ directory
    # Usage steps:
    # 1. Visit https://alphacephei.com/vosk/models to download corresponding model
    # 2. Extract model files to models/vosk/ folder in project directory
    # 3. Specify correct model path in configuration
    # 4. Note: VOSK Chinese model output does not include punctuation, there will be spaces between words
    type: vosk
    model_path: your_model_path, e.g.: models/vosk/vosk-model-small-cn-0.22
    output_dir: tmp/
  Qwen3ASRFlash:
    # Tongyi Qianwen Qwen3-ASR-Flash speech recognition service, need to create API key on Alibaba Cloud Bailian platform first
    # Application steps:
    # 1. Log in to Alibaba Cloud Bailian platform. https://bailian.console.aliyun.com/
    # 2. Create API-KEY  https://bailian.console.aliyun.com/#/api-key
    # 3. Qwen3-ASR-Flash is based on Tongyi Qianwen multimodal foundation, supports multilingual recognition, singing recognition, noise rejection and other functions
    type: qwen3_asr_flash
    api_key: your_alibaba_cloud_bailian_api_key
    base_url: https://dashscope.aliyuncs.com/compatible-mode/v1
    model_name: qwen3-asr-flash
    output_dir: tmp/
    # ASR option configuration
    enable_lid: true  # Automatic language detection
    enable_itn: true  # Inverse text normalization
    #language: "zh"  # Language, supports zh, en, ja, ko, etc.
    context: ""  # Context information, used to improve recognition accuracy, not exceeding 10000 Tokens
  XunfeiStreamASR:
    # Xunfei streaming speech recognition service
    # Need to create application on Xunfei Open Platform first, get the following authentication information
    # Xunfei Open Platform address: https://www.xfyun.cn/
    # After creating application, get from "My Applications":
    # - APPID
    # - APISecret  
    # - APIKey
    type: xunfei_stream
    # Required parameters - Xunfei Open Platform application information
    app_id: your_appid
    api_key: your_apikey
    api_secret: your_apisecret
    # Recognition parameter configuration
    domain: slm # Recognition domain, iat: daily language, medical: medical, finance: finance, etc.
    language: zh_cn # Language, zh_cn: Chinese, en_us: English
    accent: mandarin # Dialect, mandarin: Mandarin
    dwa: wpgs # Dynamic correction, wpgs: real-time return of intermediate results
    # Adjust audio processing parameters to improve long speech recognition quality
    output_dir: tmp/
  
VAD:
  SileroVAD:
    type: silero
    threshold: 0.5
    threshold_low: 0.3
    model_dir: models/snakers4_silero-vad
    min_silence_duration_ms: 200  # If speech pauses are relatively long, you can set this value larger

LLM:
  # All openai types can modify hyperparameters, using AliLLM as an example
  # Currently supported types are openai, dify, ollama, can be adapted as needed
  AliLLM:
    # Define LLM API type
    type: openai
    # You can find your api_key here https://bailian.console.aliyun.com/?apiKey=1#/api-key
    base_url: https://dashscope.aliyuncs.com/compatible-mode/v1
    model_name: qwen-turbo
    api_key: your_deepseek_web_key
    temperature: 0.7  # Temperature value
    max_tokens: 500   # Maximum generated token count
    top_p: 1
    frequency_penalty: 0  # Frequency penalty
  AliAppLLM:
    # Define LLM API type
    type: AliBL
    base_url: https://dashscope.aliyuncs.com/compatible-mode/v1
    app_id: your_app_id
    # You can find your api_key here https://bailian.console.aliyun.com/?apiKey=1#/api-key
    api_key: your_api_key
    # Whether to not use local prompt: true|false (if not using, please set prompt in Bailian application)
    is_no_prompt: true
    # Ali_memory_id: false (not used) | your_memory_id (please get from Bailian application settings)
    # Tips!: Ali_memory has not implemented multi-user memory storage (memory called by id)
    ali_memory_id: false
  DoubaoLLM:
    # Define LLM API type
    type: openai
    # First activate service, open the following URL, search for Doubao-1.5-pro in activated services, activate it
    # Activation address: https://console.volcengine.com/ark/region:ark+cn-beijing/openManagement?LLM=%7B%7D&OpenTokenDrawer=false
    # Free quota 500000 tokens
    # After activation, go here to get key: https://console.volcengine.com/ark/region:ark+cn-beijing/apiKey?apikey=%7B%7D
    base_url: https://ark.cn-beijing.volces.com/api/v3
    model_name: doubao-1-5-pro-32k-250115
    api_key: your_doubao_web_key
  DeepSeekLLM:
    # Define LLM API type
    type: openai
    # You can find your api key here https://platform.deepseek.com/
    model_name: deepseek-chat
    url: https://api.deepseek.com
    api_key: your_deepseek_web_key
  ChatGLMLLM:
    # Define LLM API type
    type: openai
    # glm-4-flash is free, but still needs registration and api_key
    # You can find your api key here https://bigmodel.cn/usercenter/proj-mgmt/apikeys
    model_name: glm-4-flash
    url: https://open.bigmodel.cn/api/paas/v4/
    api_key: your_chat_glm_web_key
  OllamaLLM:
    # Define LLM API type
    type: ollama
    model_name: qwen2.5 #  Model name to use, need to download using ollama pull first
    base_url: http://localhost:11434  # Ollama service address
  DifyLLM:
    # Define LLM API type
    type: dify
    # Recommend using locally deployed dify interface, some regions in China may have restricted access to dify public cloud interface
    # If using DifyLLM, prompt in configuration file is invalid, need to set prompt in dify console
    base_url: https://api.dify.ai/v1
    api_key: your_difyllm_web_key
    # Conversation mode to use, can choose workflow workflows/run, conversation mode chat-messages, text generation completion-messages
    # When using workflows for return, input parameter is query, return parameter name should be set to answer
    # Text generation default input parameter is also query
    mode: chat-messages
  GeminiLLM:
    type: gemini
    # Google Gemini API, need to create API key in Google Cloud console first and get api_key
    # If using within China, please comply with "Interim Measures for the Management of Generative Artificial Intelligence Services"
    # Token application address: https://aistudio.google.com/apikey
    # If deployment location cannot access interface, need to enable VPN
    api_key: your_gemini_web_key
    model_name: "gemini-2.0-flash"
    http_proxy: ""  #"http://127.0.0.1:10808"
    https_proxy: "" #http://127.0.0.1:10808"
  CozeLLM:
    # Define LLM API type
    type: coze
    # You can find personal token here
    # https://www.coze.cn/open/oauth/pats
    # bot_id and user_id content should be written within quotes
    bot_id: "your_bot_id"
    user_id: "your_user_id"
    personal_access_token: your_coze_personal_token
  VolcesAiGatewayLLM:
    # Volcano Engine - Edge Large Model Gateway
    # Define LLM API type
    type: openai
    # First activate service, open the following URL, create gateway access key, search and check Doubao-pro-32k-functioncall, activate
    # If you need to use speech synthesis provided by edge large model gateway, also check Doubao-Speech Synthesis, see TTS.VolcesAiGatewayTTS configuration
    # https://console.volcengine.com/vei/aigateway/
    # After activation, go here to get key: https://console.volcengine.com/vei/aigateway/tokens-list
    base_url: https://ai-gateway.vei.volces.com/v1
    model_name: doubao-pro-32k-functioncall
    api_key: your_gateway_access_key
  LMStudioLLM:
    # Define LLM API type
    type: openai
    model_name: deepseek-r1-distill-llama-8b@q4_k_m # Model name to use, need to download from community first
    url: http://localhost:1234/v1 # LM Studio service address
    api_key: lm-studio # LM Studio service fixed API Key
  HomeAssistant:
    # Define LLM API type
    type: homeassistant
    base_url: http://homeassistant.local:8123
    agent_id: conversation.chatgpt
    api_key: your_home_assistant_api_access_token
  FastgptLLM:
    # Define LLM API type
    type: fastgpt
    # If using fastgpt, prompt in configuration file is invalid, need to set prompt in fastgpt console
    base_url: https://host/api/v1
    # You can find your api_key here
    # https://cloud.tryfastgpt.ai/account/apikey
    api_key: your_fastgpt_key
    variables:
      k: "v"
      k2: "v2"
  XinferenceLLM:
    # Define LLM API type
    type: xinference
    # Xinference service address and model name
    model_name: qwen2.5:72b-AWQ  # Model name to use, need to start corresponding model in Xinference first
    base_url: http://localhost:9997  # Xinference service address
  XinferenceSmallLLM:
    # Define lightweight LLM API type, used for intent recognition
    type: xinference
    # Xinference service address and model name
    model_name: qwen2.5:3b-AWQ  # Small model name to use, used for intent recognition
    base_url: http://localhost:9997  # Xinference service address
# VLLM configuration (Vision Language Model)
VLLM:
  ChatGLMVLLM:
    type: openai
    # glm-4v-flash is Zhipu AI's free vision model, need to create API key on Zhipu AI platform first and get api_key
    # You can find your api key here https://bigmodel.cn/usercenter/proj-mgmt/apikeys
    model_name: glm-4v-flash  # Zhipu AI's vision model
    url: https://open.bigmodel.cn/api/paas/v4/
    api_key: your_api_key
  QwenVLVLLM:
    type: openai
    model_name: qwen2.5-vl-3b-instruct
    url: https://dashscope.aliyuncs.com/compatible-mode/v1
    # You can find your api key here https://bailian.console.aliyun.com/?apiKey=1#/api-key
    api_key: your_api_key
  XunfeiSparkLLM:
    # Define LLM API type
    type: openai
    # First create new application at the following address
    # Application activation address: https://console.xfyun.cn/app/myapp
    # Has free quota, but also need to activate service to get api_key
    # Each model needs to be activated separately, each model's api_password is different, e.g., Lite model activated at https://console.xfyun.cn/services/cbm
    base_url: https://ark.cn-beijing.volces.com/api/v3
    model_name: lite
    api_key: your_api_password
TTS:
  # Currently supported types are edge, doubao, can be adapted as needed
  EdgeTTS:
    # Define TTS API type
    type: edge
    voice: zh-CN-XiaoxiaoNeural
    output_dir: tmp/
  DoubaoTTS:
    # Define TTS API type
    type: doubao
    # Volcano Engine speech synthesis service, need to create application on Volcano Engine console first and get appid and access_token
    # Volcano Engine speech must be purchased, starting price ¥30, gives 100 concurrency. If using free version only has 2 concurrency, will frequently report tts errors
    # After purchasing service, after purchasing free voice, may need to wait about half an hour before it can be used.
    # Regular voices activated here: https://console.volcengine.com/speech/service/8
    # Wanwan Xiaohe voice activated here: https://console.volcengine.com/speech/service/10007, after activation set voice below to zh_female_wanwanxiaohe_moon_bigtts
    api_url: https://openspeech.bytedance.com/api/v1/tts
    voice: BV001_streaming
    output_dir: tmp/
    authorization: "Bearer;"
    appid: your_volcano_engine_speech_synthesis_service_appid
    access_token: your_volcano_engine_speech_synthesis_service_access_token
    cluster: volcano_tts
    speed_ratio: 1.0
    volume_ratio: 1.0
    pitch_ratio: 1.0
  # Volcano TTS, supports bidirectional streaming TTS
  HuoshanDoubleStreamTTS:
    type: huoshan_double_stream
    # Visit https://console.volcengine.com/speech/service/10007 to activate speech synthesis large model, purchase voice
    # Get appid and access_token at the bottom of the page
    # Resource ID is fixed: volc.service_type.10029 (Large model speech synthesis and mixing)
    # If using Gizwits, change interface address to wss://bytedance.gizwitsapi.com/api/v3/tts/bidirection
    # Gizwits does not need to fill appid
    ws_url: wss://openspeech.bytedance.com/api/v3/tts/bidirection
    appid: your_volcano_engine_speech_synthesis_service_appid
    access_token: your_volcano_engine_speech_synthesis_service_access_token
    resource_id: volc.service_type.10029
    speaker: zh_female_wanwanxiaohe_moon_bigtts
    speech_rate: 0
    loudness_rate: 0
    pitch: 0
  CosyVoiceSiliconflow:
    type: siliconflow
    # SiliconFlow TTS
    # Token application address https://cloud.siliconflow.cn/account/ak
    model: FunAudioLLM/CosyVoice2-0.5B
    voice: FunAudioLLM/CosyVoice2-0.5B:alex
    output_dir: tmp/
    access_token: your_siliconflow_api_key
    response_format: wav
  CozeCnTTS:
    type: cozecn
    # COZECN TTS
    # Token application address https://www.coze.cn/open/oauth/pats
    voice: 7426720361733046281
    output_dir: tmp/
    access_token: your_coze_web_key
    response_format: wav
  VolcesAiGatewayTTS:
    type: openai
    # Volcano Engine - Edge Large Model Gateway
    # First activate service, open the following URL, create gateway access key, search and check Doubao-Speech Synthesis, activate
    # If you need to use LLM provided by edge large model gateway, also check Doubao-pro-32k-functioncall, see LLM.VolcesAiGatewayLLM configuration
    # https://console.volcengine.com/vei/aigateway/
    # After activation, go here to get key: https://console.volcengine.com/vei/aigateway/tokens-list
    api_key: your_gateway_access_key
    api_url: https://ai-gateway.vei.volces.com/v1/audio/speech
    model: doubao-tts
    # Voice list see https://www.volcengine.com/docs/6561/1257544
    voice: zh_male_shaonianzixin_moon_bigtts
    speed: 1
    output_dir: tmp/
  FishSpeech:
    # Refer to tutorial: https://github.com/xinnan-tech/xiaozhi-esp32-server/blob/main/docs/fish-speech-integration.md
    type: fishspeech
    output_dir: tmp/
    response_format: wav
    reference_id: null
    reference_audio: ["config/assets/wakeup_words.wav",]
    reference_text: ["哈啰啊，我是小智啦，声音好听的台湾女孩一枚，超开心认识你耶，最近在忙啥，别忘了给我来点有趣的料哦，我超爱听八卦的啦",]
    normalize: true
    max_new_tokens: 1024
    chunk_length: 200
    top_p: 0.7
    repetition_penalty: 1.2
    temperature: 0.7
    streaming: false
    use_memory_cache: "on"
    seed: null
    channels: 1
    rate: 44100
    api_key: "your_api_key"
    api_url: "http://127.0.0.1:8080/v1/tts"
  GPT_SOVITS_V2:
    # Define TTS API type
    # Start TTS method:
    # python api_v2.py -a 127.0.0.1 -p 9880 -c GPT_SoVITS/configs/demo.yaml
    type: gpt_sovits_v2
    url: "http://127.0.0.1:9880/tts"
    output_dir: tmp/
    text_lang: "auto"
    ref_audio_path: "demo.wav"
    prompt_text: ""
    prompt_lang: "zh"
    top_k: 5
    top_p: 1
    temperature: 1
    text_split_method: "cut0"
    batch_size: 1
    batch_threshold: 0.75
    split_bucket: true
    return_fragment: false
    speed_factor: 1.0
    streaming_mode: false
    seed: -1
    parallel_infer: true
    repetition_penalty: 1.35
    aux_ref_audio_paths: []
  GPT_SOVITS_V3:
    # Define TTS API type GPT-SoVITS-v3lora-20250228
    # Start TTS method:
    # python api.py
    type: gpt_sovits_v3
    url: "http://127.0.0.1:9880"
    output_dir: tmp/
    text_language: "auto"
    refer_wav_path: "caixukun.wav"
    prompt_language: "zh"
    prompt_text: ""
    top_k: 15
    top_p: 1.0
    temperature: 1.0
    cut_punc: ""
    speed: 1.0
    inp_refs: []
    sample_steps: 32
    if_sr: false
  MinimaxTTSHTTPStream:
  # Minimax streaming speech synthesis service
    type: minimax_httpstream
    output_dir: tmp/
    group_id: your_minimax_platform_groupid
    api_key: your_minimax_platform_interface_key
    model: "speech-01-turbo"
    voice_id: "female-shaonv"
    # The following can be left unset, using default settings
    # voice_setting:
    #     voice_id: "male-qn-qingse"
    #     speed: 1
    #     vol: 1
    #     pitch: 0
    #     emotion: "happy"
    # pronunciation_dict:
    #     tone:
    #       - "处理/(chu3)(li3)"
    #       - "危险/dangerous"
    # audio_setting:
    #     sample_rate: 24000
    #     bitrate: 128000
    #     format: "mp3"
    #     channel: 1
    # timber_weights:
    #   -
    #     voice_id: male-qn-qingse
    #     weight: 1
    #   -
    #     voice_id: female-shaonv
    #     weight: 1
    # language_boost: auto
  AliyunTTS:
    # Alibaba Cloud Intelligent Speech Interaction Service, need to activate service on Alibaba Cloud platform first, then get verification information
    # Platform address: https://nls-portal.console.aliyun.com/
    # appkey address: https://nls-portal.console.aliyun.com/applist
    # token address: https://nls-portal.console.aliyun.com/overview
    # Define TTS API type
    type: aliyun
    output_dir: tmp/
    appkey: your_alibaba_cloud_intelligent_speech_interaction_service_project_appkey
    token: your_alibaba_cloud_intelligent_speech_interaction_service_accesstoken, temporary 24 hours, for long-term use use access_key_id and access_key_secret below
    voice: xiaoyun
    access_key_id: your_alibaba_cloud_account_access_key_id
    access_key_secret: your_alibaba_cloud_account_access_key_secret

    # The following can be left unset, using default settings
    # format: wav
    # sample_rate: 16000
    # volume: 50
    # speech_rate: 0
    # pitch_rate: 0
  AliyunStreamTTS:
    # Alibaba Cloud CosyVoice large model streaming text-to-speech synthesis
    # Uses FlowingSpeechSynthesizer interface, supports lower latency and more natural speech quality
    # Streaming text-to-speech synthesis only provides commercial version, trial version not supported, for details see trial and commercial versions. To use this feature, please activate commercial version.
    # Supports Dragon series dedicated voices: longxiaochun, longyu, longchen, etc.
    # Platform address: https://nls-portal.console.aliyun.com/
    # appkey address: https://nls-portal.console.aliyun.com/applist
    # token address: https://nls-portal.console.aliyun.com/overview
    # Uses three-stage streaming interaction: StartSynthesis -> RunSynthesis -> StopSynthesis
    type: aliyun_stream
    output_dir: tmp/
    appkey: your_alibaba_cloud_intelligent_speech_interaction_service_project_appkey
    token: your_alibaba_cloud_intelligent_speech_interaction_service_accesstoken, temporary 24 hours, for long-term use use access_key_id and access_key_secret below
    voice: longxiaochun 
    access_key_id: your_alibaba_cloud_account_access_key_id
    access_key_secret: your_alibaba_cloud_account_access_key_secret
    # As of July 21, 2025, large model voices only use Beijing node, other nodes not yet supported
    host: nls-gateway-cn-beijing.aliyuncs.com
    # The following can be left unset, using default settings
    # format: pcm  # Audio format: pcm, wav, mp3
    # sample_rate: 16000  # Sample rate: 8000, 16000, 24000
    # volume: 50  # Volume: 0-100
    # speech_rate: 0  # Speech rate: -500 to 500
    # pitch_rate: 0  # Pitch: -500 to 500
  TencentTTS:
    # Tencent Cloud Intelligent Speech Interaction Service, need to activate service on Tencent Cloud platform first
    # appid, secret_id, secret_key application address: https://console.cloud.tencent.com/cam/capi
    # Free resource claim: https://console.cloud.tencent.com/tts/resourcebundle
    type: tencent
    output_dir: tmp/
    appid: your_tencent_cloud_appid
    secret_id: your_tencent_cloud_secretid
    secret_key: your_tencent_cloud_secretkey
    region: ap-guangzhou
    voice: 101001

  TTS302AI:
    # 302AI speech synthesis service, need to create account and recharge on 302 platform first, then get key information
    # Add 302.ai TTS configuration
    # Token application address: https://dash.302.ai/
    # Get api_key path: https://dash.302.ai/apis/list
    # Price, $35/million characters. Volcano original version ¥450/million characters
    type: doubao
    api_url: https://api.302ai.cn/doubao/tts_hd
    authorization: "Bearer "
    # Wanwan Xiaohe voice
    voice: "zh_female_wanwanxiaohe_moon_bigtts"
    output_dir: tmp/
    access_token: "your_302_api_key"
  GizwitsTTS:
    type: doubao
    # Volcano Engine as base, can fully use enterprise-level Volcano Engine speech synthesis service
    # First 10,000 registered users will receive ¥5 trial amount
    # Get API Key address: https://agentrouter.gizwitsapi.com/panel/token
    api_url: https://bytedance.gizwitsapi.com/api/v1/tts
    authorization: "Bearer "
    # Wanwan Xiaohe voice
    voice: "zh_female_wanwanxiaohe_moon_bigtts"
    output_dir: tmp/
    access_token: "your_gizwits_api_key"
  ACGNTTS:
    # Online website: https://acgn.ttson.cn/
    # Token purchase: www.ttson.cn
    # Development related questions please submit to QQ on the website
    # Character id acquisition address: ctrl+f quick search character - website administrator does not allow publishing, can ask website administrator
    # Parameter meanings see development documentation: https://www.yuque.com/alexuh/skmti9/wm6taqislegb02gd?singleDoc#
    type: ttson
    token: your_token
    voice_id: 1695
    speed_factor: 1
    pitch_factor: 0
    volume_change_dB: 0
    to_lang: ZH
    url: https://u95167-bd74-2aef8085.westx.seetacloud.com:8443/flashsummary/tts?token=
    format: mp3
    output_dir: tmp/
    emotion: 1
  OpenAITTS:
    # OpenAI official text-to-speech service, supports most languages worldwide
    type: openai
    # You can get api key here
    # https://platform.openai.com/api-keys
    api_key: your_openai_api_key
    # Need to use proxy in China
    api_url: https://api.openai.com/v1/audio/speech
    # Can choose tts-1 or tts-1-hd, tts-1 is faster, tts-1-hd has better quality
    model: tts-1
    # Speaker, can choose alloy, echo, fable, onyx, nova, shimmer
    voice: onyx
    # Speech rate range 0.25-4.0
    speed: 1
    output_dir: tmp/
  CustomTTS:
    # Custom TTS interface service, request parameters can be customized, can connect to many TTS services
    # Using locally deployed KokoroTTS as an example
    # If only CPU: docker run -p 8880:8880 ghcr.io/remsky/kokoro-fastapi-cpu:latest
    # If only GPU: docker run --gpus all -p 8880:8880 ghcr.io/remsky/kokoro-fastapi-gpu:latest
    # Requires interface to use POST method request and return audio file
    type: custom
    method: POST
    url: "http://127.0.0.1:8880/v1/audio/speech"
    params: # Custom request parameters
      input: "{prompt_text}"
      response_format: "mp3"
      download_format: "mp3"
      voice: "zf_xiaoxiao"
      lang_code: "z"
      return_download_link: true
      speed: 1
      stream: false
    headers: # Custom request headers
      # Authorization: Bearer xxxx
    format: mp3 # Audio format returned by interface
    output_dir: tmp/
  LinkeraiTTS:
    type: linkerai
    api_url: https://tts.linkerai.cn/tts
    audio_format: "pcm"
    # Default access_token is provided for free testing, this access_token should not be used for commercial purposes
    # If the effect is good, you can apply for your own token, application address: https://linkerai.cn
    # Parameter meanings see development documentation: https://tts.linkerai.cn/docs
    # Supports voice cloning, you can upload audio yourself, fill in voice parameter, when voice parameter is empty, uses default voice
    access_token: "U4YdYXVfpwWnk2t5Gp822zWPCuORyeJL"
    voice: "OUeAo1mhq6IBExi"
    output_dir: tmp/
  PaddleSpeechTTS:
    # Baidu PaddlePaddle PaddleSpeech supports local offline deployment, supports model training
    # Framework address https://www.paddlepaddle.org.cn/
    # Project address https://github.com/PaddlePaddle/PaddleSpeech
    # SpeechServerDemo https://github.com/PaddlePaddle/PaddleSpeech/tree/develop/demos/speech_server
    # For streaming transmission refer to https://github.com/PaddlePaddle/PaddleSpeech/wiki/PaddleSpeech-Server-WebSocket-API
    type: paddle_speech
    protocol: websocket # protocol choices = ['websocket', 'http']
    url: ws://127.0.0.1:8092/paddlespeech/tts/streaming  # TTS service URL address, points to local server [websocket default ws://127.0.0.1:8092/paddlespeech/tts/streaming, http default http://127.0.0.1:8090/paddlespeech/tts]
    spk_id: 0  # Speaker ID, 0 usually represents default speaker
    sample_rate: 24000  # Sample rate [websocket default 24000, http default 0 auto select]
    speed: 1.0  # Speech rate, 1.0 represents normal speed, >1 means faster, <1 means slower
    volume: 1.0  # Volume, 1.0 represents normal volume, >1 means increase, <1 means decrease
    save_path:   # Save path
  IndexStreamTTS:
    # TTS interface service based on Index-TTS-vLLM project
    # Refer to tutorial: https://github.com/Ksuriuri/index-tts-vllm/blob/master/README.md
    type: index_stream
    api_url: http://127.0.0.1:11996/tts
    audio_format: "pcm"
    # Default voice, if you need other voices, register in project assets folder
    voice: "jay_klee"
    output_dir: tmp/
  AliBLTTS:
    # Alibaba Bailian CosyVoice large model streaming text-to-speech synthesis
    # You can find your api_key here https://bailian.console.aliyun.com/?apiKey=1#/api-key
    # cosyvoice-v3 and some voices require application activation
    type: alibl_stream
    api_key: your_api_key
    model: "cosyvoice-v2"
    voice: "longcheng_v2"
    output_dir: tmp/
    # The following can be left unset, using default settings
    # format: pcm  # Audio format: pcm, wav, mp3, opus
    # sample_rate: 24000  # Sample rate: 16000, 24000, 48000
    # volume: 50  # Volume: 0-100
    # rate: 1  # Speech rate: 0.5~2
    # pitch: 1  # Pitch: 0.5~2
  XunFeiTTS:
    # Xunfei TTS service Official website: https://www.xfyun.cn/
    # Log in to Xunfei Speech Technology Platform https://console.xfyun.cn/app/myapp to create related applications
    # Select needed service to get api related configuration https://console.xfyun.cn/services/uts
    # Purchase related services for applications (APPID) that need to be used, e.g.: Ultra-realistic synthesis https://console.xfyun.cn/services/uts
    type: xunfei_stream
    api_url: wss://cbm01.cn-huabei-1.xf-yun.com/v1/private/mcd9m97e6
    app_id: your_app_id
    api_secret: your_api_secret
    api_key: your_api_key
    voice: x5_lingxiaoxuan_flow
    output_dir: tmp/
    # The following can be left unset, using default settings, note V5 voices do not support colloquial configuration
    # oral_level: mid  # Colloquial level: high, mid, low
    # spark_assist: 1  # Whether to use large model for colloquialization Enable:1, Disable:0
    # stop_split: 0  # Disable server-side sentence splitting Not disabled: 0, Disabled: 1
    # remain: 0  # Whether to retain original written language appearance Retain:1, Don't retain:0
    # format: raw  # Audio format: raw(PCM), lame(MP3), speex, opus, opus-wb, opus-swb, speex-wb
    # sample_rate: 24000  # Sample rate: 16000, 8000, 24000
    # volume: 50  # Volume: 0-100
    # speed: 50  # Speech rate: 0-100
    # pitch: 50  # Pitch: 0-100
